{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "## Brief\n",
    "\n",
    "Write the Python codes for the following questions.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Paste the answer as Python in the answer code section below each question.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "Question: The scraping of `https://www.scrapethissite.com/pages/forms/` in the last section assumes a hardcoded (fixed) no of pages. Can you improve the code by removing the hardcoded no of pages and instead use the `Â»` button to determine if there are more pages to scrape? Hint: Use a `while` loop.\n",
    "\n",
    "```python\n",
    "def parse_and_extract_rows(soup: BeautifulSoup):\n",
    "    \"\"\"\n",
    "    Extract table rows from the parsed HTML.\n",
    "\n",
    "    Args:\n",
    "        soup: The parsed HTML.\n",
    "\n",
    "    Returns:\n",
    "        An iterator of dictionaries with the data from the current page.\n",
    "    \"\"\"\n",
    "    header = soup.find('tr')\n",
    "    headers = [th.text.strip() for th in header.find_all('th')]\n",
    "    teams = soup.find_all('tr', 'team')\n",
    "    for team in teams:\n",
    "        row_dict = {}\n",
    "        for header, col in zip(headers, team.find_all('td')):\n",
    "            row_dict[header] = col.text.strip()\n",
    "        yield row_dict\n",
    "```\n",
    "\n",
    "Answer:\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "## Submission\n",
    "\n",
    "- Submit the URL of the GitHub Repository that contains your work to NTU black board.\n",
    "- Should you reference the work of your classmate(s) or online resources, give them credit by adding either the name of your classmate or URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_and_extract_rows(response_text: str):\n",
    "    \"\"\"\n",
    "    Parse the HTML and extract table rows from the response object's text.\n",
    "    \n",
    "    Args:\n",
    "        response_text (str): The HTML text from the response object.\n",
    "        \n",
    "    Returns:\n",
    "        An iterator of dictionaries with the data from the current page.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(response_text, \"html.parser\")\n",
    "    header = soup.find('tr')\n",
    "    headers = [th.text.strip() for th in header.find_all('th')]\n",
    "    teams = soup.find_all('tr', 'team')\n",
    "    for team in teams:\n",
    "        row_dict = {}\n",
    "        for header, col in zip(headers, team.find_all('td')):\n",
    "            row_dict[header] = col.text.strip()\n",
    "        yield row_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1...\n",
      "Fetching page 2...\n",
      "Fetching page 3...\n",
      "Fetching page 4...\n",
      "Fetching page 5...\n",
      "Fetching page 6...\n",
      "Fetching page 7...\n",
      "Fetching page 8...\n",
      "Fetching page 9...\n",
      "Fetching page 10...\n",
      "Fetching page 11...\n",
      "Fetching page 12...\n",
      "Fetching page 13...\n",
      "Fetching page 14...\n",
      "Fetching page 15...\n",
      "Fetching page 16...\n",
      "Fetching page 17...\n",
      "Fetching page 18...\n",
      "Fetching page 19...\n",
      "Fetching page 20...\n",
      "Fetching page 21...\n",
      "Fetching page 22...\n",
      "Fetching page 23...\n",
      "Fetching page 24...\n",
      "Total rows: 582\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "rows = []\n",
    "page_num = 0\n",
    "page_exists = True\n",
    "\n",
    "while page_exists:\n",
    "    page_num += 1\n",
    "    print(f\"Fetching page {page_num}...\")\n",
    "\n",
    "    r = requests.get(f\"https://www.scrapethissite.com/pages/forms/?page_num={page_num}\")\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    for row_dict in parse_and_extract_rows(r.text):\n",
    "        rows.append(row_dict)\n",
    "\n",
    "    # pause for 1 second between requests\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Check if next page exists\n",
    "    next_button = soup.find('a', {'aria-label': 'Next'})\n",
    "\n",
    "    if not next_button:\n",
    "        page_exists = False # stop while loop if no next button is found\n",
    "\n",
    "print(f\"Total rows: {len(rows)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bde",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
